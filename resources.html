<!DOCTYPE HTML>
<!--
	Helios by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Resources</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="no-sidebar is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

<!-- 					
						<div class="inner">
							<header>
								<h1><a href="index.html" id="logo">Helios</a></h1>
							</header>
						</div> -->

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="people.html">People</a>
									<ul>
										<li><a href="people.html">Current group members</a></li>
										<li><a href="alumni.html">Former group members</a></li>
									</ul>
								</li>
								<li><a href="research.html">Research</a></li>
								<li><a href="contact.html">Contact</a></li>
								<li>
									<a href="#">Internal links</a>
									<ul>
										<li><a href="resources.html">Cluster Resources</a></li>
										<li>
											<a href="#">Group Meetings &hellip;</a>
											<ul>
												<li><a href="https://docs.google.com/spreadsheets/d/1YFsH6fzITi7bT16eGPrzLqdHAFyn1Xpc47T01iV1myU/edit#gid=517585460">Schedule</a></li>
												<li><a href="https://www.google.com/url?q=https://sites.google.com/cfa.harvard.edu/hernslist/home">Mentor collab series</a></li>
												<li><a href="https://drive.google.com/file/d/1YxDjaaM9P0AW9JwbgO7wflyHqWIwn24S/view">Community Agreement</a></li>
											</ul>
										</li>
									</ul>
								</li>
							</ul>
						</nav>

				</div>

			<!-- Main -->
				<div class="wrapper style1">

					<div class="container">
						<article id="main" class="special">
							<h2 id="hernquist-group-cluster-resources">Hernquist Group Cluster Resources</h2>
								<p>The cluster used in the Hernquist group is called Cannon. It is managed by FAS Research Computing (RC).
								</p>

								<p>First it is a good idea to familiarize yourself with documentation provided by RC.
								</p>

								<ul>
									<li><a href="https://docs.rc.fas.harvard.edu/kb/quickstart-guide/">Quickstart Guide</a>. Contains information about requesting an account, logging into the cluster, file transfer, and submitting simple jobs.</li>
									<li><a href="https://docs.rc.fas.harvard.edu/kb/running-jobs/">Running Jobs</a>. Contains more detailed information about running jobs, e.g., running batch jobs. Also contains a section on the different partitions available. The partitions in this link are available to all cluster users. Members of the hernquist group have access to more partitions, outlined below.</li>
									<li><a href="https://docs.rc.fas.harvard.edu/kb/cluster-storage/">Cluster Storage</a>. </li>
									<li><a href="https://docs.rc.fas.harvard.edu/kb/transferring-data-on-the-cluster/">Transferring Files</a>. More information on transferring files on the cluster and between the cluster and your local machine, including <code>rsync</code> and <code>fpsync</code>.</li>
									<li><a href="https://docs.rc.fas.harvard.edu/kb/vdi-apps/">VDI Apps</a>. If you&#39;d like, e.g., a jupyter notebook on the cluster, you can do this very conveniently in a web browser. Other applications are also available.</li>
									<li><a href="https://docs.rc.fas.harvard.edu/kb/common-pitfalls/?seq_no=2">Common Cluster Pitfalls</a>.</li>
								</ul>


							<h2 id="group-partitions">Group Partitions</h2>
								<p>In addition to the partitions available in the <a href="https://docs.rc.fas.harvard.edu/kb/running-jobs/">Running Jobs</a> document, we have access to the following partitions.</p>

								<table>
								<thead>
									<tr>
										<td>Partition</td>
										<td>Nodes</td>
										<td>Cores per node</td>
										<td>CPU Core Types</td>
										<td>Mem per Node (GB)</td>
										<td>Time Limit</td>
										<td>Max Jobs</td>
										<td>Max Cores</td>
										<td>MPI Suitable?</td>
										<td>GPU Capable?</td>
									</tr>
								</thead>
								<tbody>
								<tr>
									<td>hernquist</td>
									<td>10</td>
									<td>48</td>
									<td>Intel Cascade Lake</td>
									<td>184</td>
									<td>7 days</td>
									<td>none</td>
									<td>none</td>
									<td>Yes</td>
									<td>No</td>
								</tr>
								<tr>
									<td>hernquist_ice</td>
									<td>12</td>
									<td>64</td>
									<td>Intel Ice Lake</td>
									<td>499</td>
									<td>7 days</td>
									<td>none</td>
									<td>none</td>
									<td>Yes</td>
									<td>No</td>
								</tr>
								<tr>
									<td>itc_cluster</td>
									<td>20</td>
									<td>112</td>
									<td>Intel Sapphire Rapids</td>
									<td>1031</td>
									<td>7 days</td>
									<td>none</td>
									<td>none</td>
									<td>Yes</td>
									<td>No</td>
								</tr>
								<tr>
									<td>itc_gpu</td>
									<td>4</td>
									<td>64</td>
									<td>Intel Ice Lake</td>
									<td>2063</td>
									<td>7 days</td>
									<td>none</td>
									<td>none</td>
									<td>Yes</td>
									<td>Yes (4 A100/node)</td>
									</tr>
								</tbody>
								</table>



							<h2 id="group-storage">Group Storage</h2>
								<p>There is a wide variety of storage options available. They are (replace <code>abeane</code> with your username):
								</p>
								<ul>
									<li><code>home</code>: <code>/n/homeN/abeane</code>. <code>N</code> is a number from <code>01</code> to <code>15</code>. Note that <code>~</code> is a shortcut for your home directory, and you can find the exact path to your home directory with <code>pwd</code>. Home directories have a hard limit of <code>100 GB</code>. Home directories <em>are backed up</em>. They are suitable for placing scripts used to generate data, installing software, etc.</li>
									<li><code>scratch</code>: <code>/n/holyscratch01/hernquist_lab/abeane</code>. Scratch storage is the highest performance storage available, but it is not backed up and <strong>files older than 90 days are deleted</strong>. The lab has a quota of <code>50 TB</code>.</li>
									<li><code>holystore01</code>: <code>/n/holystore01/LABS/hernquist_lab/Users/abeane</code>. <code>holystore01</code> is a larger file storage system. This system is suitable for long-term storage of large data products. It has a total capacity of <code>600 TB</code>, but at the time of writing <code>587 TB</code> is used. The total capacity of this storage is fixed and cannot be increased. Data is not backed up.</li>
									<li><code>holylfs05</code>: <code>/n/holylfs05/LABS/hernquist_lab/Users/abeane</code>. <code>holylfs05</code> is mostly identical to <code>holystore01</code>. It has a total capacity of <code>300 TB</code>, of which <code>222 TB</code> is used at the time of writing. Unlike <code>holystore01</code>, our storage limit on <code>holylfs05</code> can be increased in the future if needed. Data is not backed up.</li>
								</ul>

								<p>More information can be found in the <a href="https://docs.rc.fas.harvard.edu/kb/cluster-storage/">Cluster Storage</a> page.
								</p>

								<p>For <code>holystore01</code> and <code>holylfs05</code>, if you do not have a directory in the <code>Users</code> directory, email RC (<a href="mailto:rchelp@fas.rc.edu)">rchelp@fas.rc.edu </a>) to have them create one for you. There is also a <code>Lab</code> and <code>Everyone</code> directory in this storage. The idea is that files in <code>Users</code> are only accessible to you, in <code>Lab</code> accessible to people in the group, and <code>Everyone</code> to everyone on the cluster. I find this annoying and I just make my <code>Users</code> directory group accessible.
								</p>

								<p>There also exists on each node a local scratch directory of size <code>200-300 GB</code>. It is located at <code>/scratch</code>. It is the highest performance storage available, but data on it does not persist beyond the job. This can be used for extremely high I/O intensive jobs, but data generated must be moved somewhere else during the job. It is also good practice to delete any files you create in scratch before your job completes.
								</p>

								<p>Tape storage is in principle available from RC. This is suitable for data that needs to be kept but does not need to be used in the near future. Our group has not used tape storage yet. If you have a large amount of data (<code>&gt;10 TB</code>) that would be a good candidate for tape storage, please <a href="mailto:angus.beane@cfa.harvard.edu">reach out</a>.
								</p>

								<p>Your personal usage on each storage system can be checked with standard <code>df</code> commands. To check group usage on <code>holystore01</code> and <code>holylfs05</code>, you can use the command <code>lfs quota -hg hernquist_lab /n/holylfs05</code>. 
								</p>


						</article>
						</div>
					</div>

				</div>

			<!-- Footer -->
				<div id="footer">
					<div class="container">
						<div class="row">



								<!-- Copyright -->
									<div class="copyright">
										<ul class="menu">
											<li>&copy; Hernquist Group. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
										</ul>
									</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>